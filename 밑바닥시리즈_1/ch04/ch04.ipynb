{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5*np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.1, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "sum_squares_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.8, 0.0, 0.11, 0.0, 0.0, 0.6, 0.0, 0.0])\n",
    "sum_squares_error(y, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크로스 엔트로피 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "delta = 1e-7\n",
    "x = np.arange(0.0+delta, 1.1, 0.01)\n",
    "y = -np.log(x)\n",
    "plt.ylim(0,5)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum( t*np.log(y + delta) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.1, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.99, 0.0, 0.11, 0.0, 0.0, 0.6, 0.0, 0.0])\n",
    "cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "print(batch_mask)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    if y.ndim == 1 :\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum( t*np.log(y + delta) ) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수치 미분 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1e-50\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-50 # 0.000000000000000000000000000000000000000000000000001\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.float32(1e-4) # 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 \n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    return 0.01*x**2 + 0.1*x  # 0.02*5 + 0.1\n",
    "\n",
    "# 0.20000099999917254\n",
    "# 0.1999999999990898\n",
    "print(numerical_diff(function, 5))\n",
    "print(numerical_diff(function, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x \n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_diff(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "     \n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tf = tangent_line(function_1, 15)\n",
    "y2 = tf(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 편미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0**2 + 4**2\n",
    "\n",
    "numerical_diff(function_tmp1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3**2 + x1**2\n",
    "\n",
    "numerical_diff(function_tmp2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기(수치미분 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2 \n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1)\n",
    "\n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_gradient(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "     \n",
    "if __name__ == '__main__':\n",
    "    x0 = np.arange(-2, 2.5, 0.25)\n",
    "    x1 = np.arange(-2, 2.5, 0.25)\n",
    "    X, Y = np.meshgrid(x0, x1)\n",
    "    \n",
    "    X = X.flatten()\n",
    "    Y = Y.flatten()\n",
    "    \n",
    "    grad = numerical_gradient(function_2, np.array([X, Y]) )\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")#,headwidth=10,scale=40,color=\"#444444\")\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel('x0')\n",
    "    plt.ylabel('x1')\n",
    "    plt.grid()\n",
    "#     plt.legend()\n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from gradient_2d import numerical_gradient\n",
    "\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W) # (3,)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x) # (3,)\n",
    "#         print(z.shape)\n",
    "        y = softmax(z)      # (3,)\n",
    "#         print(y.shape)\n",
    "        loss = cross_entropy_error(y, t) # 스칼라 값\n",
    "#         print(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9]) # (2,)\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "#     grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09751666666666667, 0.0974\n",
      "train acc, test acc | 0.789, 0.7951\n",
      "train acc, test acc | 0.8744, 0.8781\n",
      "train acc, test acc | 0.8969833333333334, 0.9\n",
      "train acc, test acc | 0.90755, 0.9087\n",
      "train acc, test acc | 0.9140833333333334, 0.9145\n",
      "train acc, test acc | 0.9188333333333333, 0.9206\n",
      "train acc, test acc | 0.9232, 0.9239\n",
      "train acc, test acc | 0.9264166666666667, 0.9287\n",
      "train acc, test acc | 0.9289166666666666, 0.9303\n",
      "train acc, test acc | 0.9322333333333334, 0.9329\n",
      "train acc, test acc | 0.9345666666666667, 0.9333\n",
      "train acc, test acc | 0.9374333333333333, 0.9355\n",
      "train acc, test acc | 0.9397166666666666, 0.9376\n",
      "train acc, test acc | 0.9414833333333333, 0.9395\n",
      "train acc, test acc | 0.9432166666666667, 0.9417\n",
      "train acc, test acc | 0.9453833333333334, 0.9439\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqtElEQVR4nO3deXyU5bn/8c81k5nsJCFhEYKCCG5UQXHXupUWV1xarXWrbcUei7Wt9YitW7XnHJee1nqObV3qUvVoXepatC5Fqa2oiCuggogQ1ghJIGSb5fr9MQO/EIJMNJMnZL7v1yuvzLPMM99MkueaZ7nv29wdERHJXaGgA4iISLBUCEREcpwKgYhIjlMhEBHJcSoEIiI5ToVARCTHZa0QmNkdZrbKzN7bwnIzs5vMbIGZvWNme2Uri4iIbFk2jwjuAiZ+xvKjgFHpr8nA77OYRUREtiBrhcDdZwBrPmOVScCfPGUmUG5m22Urj4iIdC4vwNceCixpN12Tnre844pmNpnUUQPFxcV777LLLj0SUESkr3jjjTc+dfcBnS0LshBkzN1vBW4FGD9+vM+aNSvgRCIi2xYz+2RLy4K8a2gpMKzddHV6noiI9KAgC8ETwFnpu4f2BxrcfbPTQiIikl1ZOzVkZvcDhwFVZlYDXAlEANz9D8A04GhgAdAEnJOtLCIismVZKwTuftpWljvwg2y9voiIZEYti0VEcpwKgYhIjlMhEBHJcSoEIiI5ToVARCTHbRMti0VEtiXuTizhxJNJYnGnNZEglnBi8SRtiSRt8SSxjd89/ThBLBYjEWshEW8l0dbKulA/2jxMqHkNtK1n33FjGTusvNvzqhCISK+0YWe6YYfZ1uH7xvnpebGEE08kiSVT3+MJJ5ZMEosnU9PxOB5vI5GI4fFWPN5KI0U0WSHEWujftAiSbZCI44lWLNHGJ+EdWGlVFLetYWzLq4SSMcLJNiwZI+wxnmd/FvhQtk98wqnJpwl7jLDHiRAjSpwb4ycz14dzQGgOl+TdT5QEUWKUEidicc5tu4g5PoKvh1/i+rxbCZlv8h5MaL2e+V7NOeGnGRdawJxBt6gQiEjPSCY9tcNNJDf5FNsWT9IaT9IaT9AS2/R7omkt8dYm4rFW4m2tJGItrPd8VoYG0hpPUt3wBqF4E8lYK56IQaKNZT6A2bYrbYkk32h9jLxkC6FkjJDHCCfjvJEcxbTk/oRJcGPkZiIkiBAnQpyoxXkycQD3Jb5CKU08Fr2cCHHybMM6CW6OT+K2xLEMpZZ/Fly42c/5X3ybp8PHMtqW8Mv4jzdb/r+lP2J5yUSqbRUXNP52s+X51aOZ238vRjUt49hFs0hYHslQlEQogociTB49mDWVuzFkXYxBC7fDw/kQjqa+8qJc/aX9SZTvSFl9P1YvzieUFyUUySeUl08oks8ju59ApKSKSMPOhFvqsGE7ZOX3bal2XdsOdTon26x4G8SbIZmARAySMUjGoWx7CIVg7TJYtyK1PBmDeGpn2jxiAs1tCXzRy7BqHvG2ZhJtLSRiLcQT8N7o82mOJRi+8D6qVs/GEq3przbWWzF/2O5q2uJJTqn9H0Y3vw047o55kuU2kJ9GL6MtnuTK2I3s7gswkuCO4Xzgwzg39lMA7oxcxy6hJamdMHGixHg9uTNnxn4GwIvRHzM8tHKTH/nvPp6L86ZSEAnzZOu36e8NmyyfWXwk9wy5jGheiOve/xpRb8Ex4hYlGcpj7uATeG30RUTDcPIrJ5IMpXaiHopAOMqnw49j9a6nE/VWdnz5YiwvioUjWDiKRaLEd5xAcuSR5MUbKXzjNkKRKKFwBMvLh1AeDNsPBu0Grevg4xkQikA4DzbssPuPgOIqiLdC46qNO/DUzjwfQmEw65E/ny/KzN5w9/GdLlMhEElLJqClIbVTSH9561rahuxDS6iU+LK3CH/wV5Ita/GWtdC6Dmtbx9t7/Qf1kQEM/ejPfOnDm7FknJDHMU99v3H3v7A6XMmhy/7I12rv3OxlT6l4iIZkAd9p+iOnxh7bbPmIlntxQvxH3h85Pe+FTZbVezFjW28D4Iq8P3Fo6G3ayCNmUeIWpT5Uzi+LphLNC3FG24OMTizAzFJfoRDrIlU8sd2PiOaF+GrtXQxsW0zIQljICFmI9UVDeW/nKUTDIfb46A+UtCzH8vKxvHzCeVHi5TvS9KUzyM8LUT7/YSKJFvKi+USiqU+19BsK2++XClvzRur7xh1pBPLLoLgyNb+tKTV/G9q5bktUCKRvSCahtSG1s441Q6wJYi3Qf0e8dDBNdStIzH2KtpYm4q3ribc0kWhbz8LtjmF54WjyP53D2AU3E0o0E4q3EE40k5do4daKH/N2aHf2anyJn6+/drOXPan1Kmb7aE4KzeBXkVtopIBGClnnRTRSyI9j57PYB3FQ6F2OCb1KjDBx8ogRhlCYP4VOoC2vlL1C8xnLfAiFsXAEQhEsnMesfkcQjhSwfWIxgxMrCOVFCIcjhKIF5EULWFuxO4XRCKXeSFGeE4kWkl9YSH5+IQXRMIWRMIXp7wWRMPl5IUw7UungswqBrhFIz0vEoLkeb15D67rVrM/rT0NhNU3r6ih78w94Ux201BFqaSDSVs/rA77Oq/2+StHaBVy68OzNNndV8lzujh3OGBbyZP5lmyxr8nyue7eEaclW9rCP2CWymFbLJxYqIBaqJB4qYG0in4L8EI0Vu/Fw0Q9I5JWQyC/FI6VQUMrEfjtxdEEpRZFdeTTvBxTmRyiIhChI73j/kJfaERdEjiAaDpEfCRMNh4iEU5+8v78xzYStvDGd/o+KZJ2OCOTzScSgpQFvrqMpEWZNZDD1TTEK3r6bWOOnJJvqoaWeUGsDc6J78lTBsbS0NHHnqm9SRPMmm/pD/DiujZ9GP9bzVv5kGiim3otpoJgGL+ERP4KX8w9mYLSNk/g78Ug/PFpEOFpEOL+IxtId8ZLBlEac/raWgsJiCopKKSwopLggQnF+HiX5eRTlhymO5hEO6dOy5B4dEUjGfPFMmlZ+xLraGlrrV5BsXkNtZCjTB55FfVMb533wPQa1LabQUztzA6Yn9mNKLHVHxtv5/0WZNdHsURooptFKaIgMob6sjdKCQl4uO45ktJREQTleUA6FFQwq35Hf9h9OSTTM6/nzKSmMUpofYfuCPIrzwxyaF26X8Jgef09E+joVgr7MHVrqobk+dfcDkHzrAZo/eYPWuqX4uhXkrV/JmnAVV1fdwNL6Zn5d/yPG2EKKgWaPUk8JS5K7cdv7CykvirJH6Ev0K9qNRLQfFJRjRRXEy0Zw/XZ7UF4UYWF4Bv36lVPWrx9VhREGh0PsBHxvY6j9gngnROQzqBD0Bclk6vZDIP6P3xKf+ySsW0Fe0yrykq005FXy/YH/x9L6Zi5vvI0D7D3qvIJaylnp27Mkb3tWrWtleGUxM4b8gnllJZQPHMaAqioqS/L5alGEk/Lz0hcgv7KVMIOz/uOKSPdSIdgWJeKw4h1Y9DKJj/9BYsksfvulv/CPRes5dMW7HGDrWcUwVvkerKKc5vAgWuMJ9hxWzpv9bmRFZT+qywsZWlHImPJCSvLzmBL0zyQigVEh2BYk4oBDOELsnUewJ39IXqwRgEU+hJmJcTz8yvvsMGw4yUN+Sk1lMdXlhYyrKGRwWQH5m5xjFxHZlApBb9TuE39y0T/wRa/w7Ogr+b+1e7B2US2n+H686ruydtB+7DJqNAeOrGT68AqKovp1ikjXac/RGyQTqZasheUk65fiv9uPcNs6AD7xIfwrsS/3z24hPqiVA/Y9hIEjT+SaEf0pK4wEHFxE+gIVgqC9eR+Jp6fy8YAj+E3Rhbzy0aec33YwbydHsqx8b3YeNYoDR1Zy146VVJXkB51WRPogFYIA+czfY89M5fXkrtzy8SjeL63jsF0GUj7yV1w6spIh5YVBRxSRHKBCEJDkS78iNP0anknsw7/GXscVh+7C8Moi9REjIj1OhSAAyaZ66l++nRcTB/PhAdfyi6PGqACISGBUCHpSMkkimeTfn/qEGesu55uH78UlX91FRUBEAqVC0FMScZKPT+G1Txp4ZOXp/GTCvvzwyFFBpxIRIRR0gJwQbyP50DmE3rmff31axCVf20VFQER6DR0RZFtbE8k/n0Hooxe4OnYmQyb+hO8dsmPQqURENlIhyCZ3En8+C/vo71wSO5fdj53CWQcMDzqViMgmVAiyqDmW5Ka1h7O0bWf2n3Qe39pv+6AjiYhsRoUgG9atpPWjGXzntWHMrBnOdScfzynjhwWdSkSkUyoE3a1+Ccm7J5GsX8b8lt/wm1O+zAnjhgadSkRki1QIutPqj0jefTzN6+r4dttUrjrtUI7dY0jQqUREPpMKQXdZOZfknybR2NTCmW0/599OO5mJYzRal4j0fioE3WT9B3+npSnOGW1X8NMzjufIXQcFHUlEJCNZbVBmZhPN7AMzW2BmUztZvr2ZTTezN83sHTM7Opt5siLWzKeNrZw8ew+Oil3P1LNPUBEQkW1K1o4IzCwM3AxMAGqA183sCXef2261y4AH3f33ZrYbMA0Ynq1M3W7BCyQe/Td+zqUsWjuYP377cA7aqSroVCIiXZLNI4J9gQXuvtDd24AHgEkd1nGgX/pxGbAsi3m617yn8Pu/ycLmIuasK+Kuc/ZVERCRbVI2rxEMBZa0m64B9uuwzlXAs2Z2AVAMfKWzDZnZZGAywPbb94JGWYv+iT94FnNsJyYnLuGm7xzO+OH9g04lIvK5BN3p3GnAXe5eDRwN3GNmm2Vy91vdfby7jx8wYECPh+yoce6zJNz5bvJn/O57R6oIiMg2LZtHBEuB9s1pq9Pz2vsuMBHA3V8xswKgCliVxVxf2Jur82hN7Mnt5x3Ol6rLgo4jIvKFZPOI4HVglJmNMLMo8E3giQ7rLAaOBDCzXYECoDaLmbrFX4smMTX/5yoCItInZK0QuHscmAL8DZhH6u6gOWZ2tZkdn17tIuBcM3sbuB/4trt7tjJ1l5q6ZoZWFAUdQ0SkW2S1QZm7TyN1S2j7eVe0ezwXOCibGbpdvJWblp7C0wO+y7YWXUSkM0FfLN7mJOqW0N/rKSspCTqKiEi3UCHoovrlCwCIVu0QcBIRke6hQtBFa5d/BEC/7XYKOImISPdQIeii1k8XEfMwA4boiEBE+gYVgi76JFTNQ4lDGdq/NOgoIiLdQt1Qd9HzeYfxUuHufCsSDjqKiEi30BFBF61Y00B1RWHQMUREuo0KQVfEW7lr+STO8o4NpEVEtl0qBF0QX7OYMEmiZRp4RkT6DhWCLqhbtqENwfBgg4iIdCMVgi7Y0IagbLDaEIhI36FC0AVtqz9OtyEYHnQUEZFuo0LQBe9HxvC7xCSG9Fc/QyLSd6gQdME/bBwPFJ9BNE9vm4j0HdqjdUFr7ccML1cbPBHpW1QIMhVr4ebab3NW4rGgk4iIdCsVggy1rVmcelC+fbBBRES6mQpBhurTbQjyB4wIOImISPdSIcjQ2hUbxiHYMeAkIiLdS4UgQ23pcQgGDdERgYj0LboFJkNvFh7A/yWSXFVRHHQUEZFupSOCDL0eH8n0kuPIC+stE5G+RXu1DJWsfJ3d+rUGHUNEpNupEGQi1swv6y7mZH8u6CQiIt1OhSADbWs+ST2oUBsCEel7VAgysKYmdetoQZXuGBKRvkeFIAPr0m0IyrYbGXASEZHup0KQgbbVi2jzMAOHDg86iohIt1M7ggz8s3QiN8XL+V252hCISN+jI4IMvNsygHllhxAOWdBRRES6nY4IMrDD8qeJlo4OOoaISFboiGBrYs38dO11TPCZQScREckKFYKtaP10EQCmNgQi0kepEGzF6qXzASjUOAQi0kdltRCY2UQz+8DMFpjZ1C2sc4qZzTWzOWb2f9nM83msW7EQUBsCEem7snax2MzCwM3ABKAGeN3MnnD3ue3WGQVcChzk7nVmNjBbeT6v2OpPaPMwg6uHBx1FRCQrsnlEsC+wwN0Xunsb8AAwqcM65wI3u3sdgLuvymKez+X58m9wWuJqBpQWBh1FRCQrslkIhgJL2k3XpOe1NxoYbWb/NLOZZjaxsw2Z2WQzm2Vms2pra7MUt3PzGwtYUz6GkNoQiEgfFfTF4jxgFHAYcBpwm5mVd1zJ3W919/HuPn7AgAE9GnCvZfdxaNGiHn1NEZGelFEhMLO/mNkxZtaVwrEUGNZuujo9r70a4Al3j7n7x8CHpApD79DWxHfX384B9l7QSUREsibTHfvvgG8B883sWjPbOYPnvA6MMrMRZhYFvgk80WGdx0gdDWBmVaROFS3MMFPWNdV+DECo/w4BJxERyZ6MCoG7P+/upwN7AYuA583sX2Z2jplFtvCcODAF+BswD3jQ3eeY2dVmdnx6tb8Bq81sLjAduNjdV3+xH6n7rFm6AICCATsGnEREJHsyvn3UzCqBM4AzgTeB+4CDgbNJf6rvyN2nAdM6zLui3WMHfpL+6nUaV6YOTsqHqA2BiPRdGRUCM3sU2Bm4BzjO3ZenF/3ZzGZlK1zQYqsX0ep5DB6iU0Mi0ndlekRwk7tP72yBu4/vxjy9yhOV3+WZBfsyo7Qg6CgiIlmT6cXi3drf1mlmFWZ2fnYi9R5L6mNEK4ZipjYEItJ3ZVoIznX3+g0T6ZbA52YlUS/y1aX/w9GFc4KOISKSVZkWgrC1+1ic7kcomp1IvURbEye1PMoe4UVBJxERyapMrxE8Q+rC8C3p6fPS8/qsxlUfUwKEKnShWET6tkwLwSWkdv7/lp5+Drg9K4l6iTU1CyhB4xCISN+XUSFw9yTw+/RXTli/MtWYrGy7nQJOIiKSXZm2IxgF/BewG7DxXkp377NNbpsaVtPk+WxXrVNDItK3ZXqx+E5SRwNx4HDgT8C92QrVGzxVcTr7+Z1UFOcHHUVEJKsyLQSF7v4CYO7+ibtfBRyTvVjBq6lrZkhFqdoQiEifl+nF4tZ0F9TzzWwKqe6kS7IXK3jfWvILPiw7GPhy0FFERLIq0yOCC4Ei4IfA3qQ6nzs7W6GC5q2NHB6bwchIr+kIVUQka7Z6RJBuPHaqu/8UaATOyXqqgK1b+TH90DgEIpIbtnpE4O4JUt1N54wN4xAUDVQbAhHp+zK9RvCmmT0BPASs3zDT3f+SlVQBW79qwzgEakMgIn1fpoWgAFgNHNFungN9shDUr29jcXIA22kcAhHJAZm2LO7z1wXae7b4OP5ie/FOUd/uV09EBDJvWXwnqSOATbj7d7o9US9QU9dMdf8itSEQkZyQ6amhp9o9LgBOBJZ1f5ze4YdLfsR75UcAhwQdRUQk6zI9NfRI+2kzux94OSuJAuat69gz8R6fFqgIiEhuyLRBWUejgIHdGaS3aFieumNI4xCISK7I9BrBOja9RrCC1BgFfc6apQsoR20IRCR3ZHpqqDTbQXqLplUfA1AxVG0IRCQ3ZHRqyMxONLOydtPlZnZC1lIFaGVbPq8md2HwkGFBRxER6RGZXiO40t0bNky4ez1wZVYSBWx6/mFMDl9Dv0KNQyAiuSHTQtDZepneerpNWbKmmWH9C4OOISLSYzItBLPM7NdmNjL99WvgjWwGC8p/LDmbc/tmF0oiIp3KtBBcALQBfwYeAFqAH2QrVFC8ZS3VvozSooKtrywi0kdketfQemBqlrMEbs3yj6gEwhqHQERySKZ3DT1nZuXtpivM7G9ZSxWQuqUfAVA0cMeAk4iI9JxMTw1Vpe8UAsDd6+iDLYub0uMQ9FcbAhHJIZkWgqSZbb9hwsyG00lvpNu6xckqHk8cyODt1IZARHJHpreA/hx42cxeAoxUt5yTs5YqIP8M7cOz0WFMKogEHUVEpMdkerH4GTMbT2rn/ybwGNCcxVyBWLmmnuoKtSEQkdyS6cXi7wEvABcBPwXuAa7K4HkTzewDM1tgZlu868jMTjYzTxebwPym5jR+GL8zyAgiIj0u02sEFwL7AJ+4++HAOKD+s55gZmHgZuAoYDfgNDPbrZP1StPbfzXz2N0v2byWMtYRKu1z18BFRD5TpoWgxd1bAMws393fB3beynP2BRa4+0J3byPVEG1SJ+tdA1xHqpFaYNYsWwBAuL+6nxaR3JJpIahJtyN4DHjOzB4HPtnKc4YCS9pvIz1vIzPbCxjm7n/9rA2Z2WQzm2Vms2prazOM3DV16UJQPEiFQERyS6YXi09MP7zKzKYDZcAzX+SFzSwE/Br4dgavfytwK8D48eOzctvqhnEI+g8ZlY3Ni4j0Wl3uQdTdX8pw1aVA+xvyq9PzNigFxgAvmhnAYOAJMzve3Wd1NdcXtcBGMDN+DGcPqe7plxYRCdTnHbM4E68Do8xshJlFgW8CT2xY6O4N7l7l7sPdfTgwEwikCAC8ltyF2wu/Q0G0T/auLSKyRVkrBO4eB6YAfwPmAQ+6+xwzu9rMjs/W635ezbUL2bE8m3VRRKR3yurHX3efBkzrMO+KLax7WDazbM01K6fwTvkRwBFBxhAR6XH6CAwkmhsoYx2JfupjSERyjwoB8Gm6++k8jUMgIjlIhQCoX7qhDYHGIRCR3KNCADRvHIdAbQhEJPeoEADvRvfgF/EzGbzd0K2vLCLSx6gQAG+1DuXpohPJj6gNgYjkHhUCoHDlG+zRrzHoGCIigVAhAP599eWcFX8k6BgiIoHI+UIQW19HPxpJlG2/9ZVFRPqgnC8Eqze0IahUGwIRyU05Xwg2jENQOnBkwElERIKR84WgZcM4BNU7BZxERCQYOV8IZhUeyPmxHzFosNoQiEhuyvlCMLepjLdLDyWSFw46iohIIHK+EAxZ/hwHliwPOoaISGByvhCcV/8bJiWeCzqGiEhgcroQtDauoR/rSZZpHAIRyV05XQg+rUndOhqpHB5sEBGRAOV0IahPtyEo0TgEIpLDcroQtNQuAqBSbQhEJIfldL/L/yyZwH/ESnhw4JCgo4iIBCanjwgWrM2jtmwMeWpDICI5LKcLwe5LH+JrRR8GHUNEJFA5XQi+1XgnhydfDTqGiEigcrYQtKxbQylNJDUOgYjkuJwtBLVLUqeEolUah0BEclvOFoKG5akBaUoGaRwCEcltOVsIWmpT4xBUVY8KOImISLBythA83+8kDordzIABg4OOIiISqJwtBEvqW4iUDyUUztm3QEQEyOFCcHDNbZxc8HrQMUREApezheDYpsfYm3lBxxARCVxOFoKmtasppQlXGwIRkdwsBJ8uTrUh0DgEIiJZLgRmNtHMPjCzBWY2tZPlPzGzuWb2jpm9YGY90rqrfkV6HILB6n5aRCRrhcDMwsDNwFHAbsBpZrZbh9XeBMa7+x7Aw8D12crTXmNdLTEPM2CYCoGISDaPCPYFFrj7QndvAx4AJrVfwd2nu3tTenImUJ3FPBtNL5rIHol7qKoa1BMvJyLSq2WzEAwFlrSbrknP25LvAk93tsDMJpvZLDObVVtb+4WD1dQ1s11FMRbKyUskIiKb6BV7QjM7AxgP3NDZcne/1d3Hu/v4AQMGfOHXO27Jrzg77/kvvB0Rkb4gm0NVLgWGtZuuTs/bhJl9Bfg5cKi7t2YxT4o7h7RO593Soqy/lIjItiCbRwSvA6PMbISZRYFvAk+0X8HMxgG3AMe7+6osZtloXcOnlNIMakMgIgJksRC4exyYAvwNmAc86O5zzOxqMzs+vdoNQAnwkJm9ZWZPbGFz3aZ2yXxA4xCIiGyQzVNDuPs0YFqHeVe0e/yVbL5+Z9amxyEoHaxxCEREIMuFoDeqW7eeJckBDNA4BCK9ViwWo6amhpaWlqCjbHMKCgqorq4mEolk/JycKwT/iH6Z8304c9WGQKTXqqmpobS0lOHDh2NmQcfZZrg7q1evpqamhhEjRmT8vF5x+2hPqqlrYlj/Qv1xifRiLS0tVFZW6v+0i8yMysrKLh9J5dwRwdlLLqO2eBRwaNBRROQzqAh8Pp/nfcutIwJ39oy9xeBI09bXFRHJETlVCNbW1VKiNgQishX19fX87ne/+1zPPfroo6mvr+/eQFmWU4Vg1ZLUOATRqswvoohI7vmsQhCPxz/zudOmTaO8vDwLqbInp64RrFuxEIB+26kNgci24hdPzmHusrXdus3dhvTjyuN23+LyqVOn8tFHHzF27FgmTJjAMcccw+WXX05FRQXvv/8+H374ISeccAJLliyhpaWFCy+8kMmTJwMwfPhwZs2aRWNjI0cddRQHH3ww//rXvxg6dCiPP/44hYWFm7zWk08+yS9/+Uva2tqorKzkvvvuY9CgQTQ2NnLBBRcwa9YszIwrr7ySk08+mWeeeYaf/exnJBIJqqqqeOGFF77w+5FThWBlE7yW3Jmdh6kNgYhs2bXXXst7773HW2+9BcCLL77I7Nmzee+99zbelnnHHXfQv39/mpub2WeffTj55JOprKzcZDvz58/n/vvv57bbbuOUU07hkUce4YwzzthknYMPPpiZM2diZtx+++1cf/31/Pd//zfXXHMNZWVlvPvuuwDU1dVRW1vLueeey4wZMxgxYgRr1qzplp83pwrBzPDePGzX8G7FF+/BVER6xmd9cu9J++677yb35t900008+uijACxZsoT58+dvVghGjBjB2LFjAdh7771ZtGjRZtutqanh1FNPZfny5bS1tW18jeeff54HHnhg43oVFRU8+eSTfPnLX964Tv/+/bvlZ8upawQ1dU1UV6gNgYh0XXFx8cbHL774Is8//zyvvPIKb7/9NuPGjev03v38/PyNj8PhcKfXFy644AKmTJnCu+++yy233BJIa+qcKgQ/WTyFi5N3BB1DRHq50tJS1q1bt8XlDQ0NVFRUUFRUxPvvv8/MmTM/92s1NDQwdGhqzK6777574/wJEyZw8803b5yuq6tj//33Z8aMGXz88ccA3XZqKGcKgSeT7BBfRHF+5v1viEhuqqys5KCDDmLMmDFcfPHFmy2fOHEi8XicXXfdlalTp7L//vt/7te66qqr+MY3vsHee+9NVVXVxvmXXXYZdXV1jBkzhj333JPp06czYMAAbr31Vk466ST23HNPTj311M/9uu2Zu3fLhnrK+PHjfdasWV1+Xv2nKyj/352ZOfqn7P+ty7OQTES6y7x589h1112DjrHN6uz9M7M33H18Z+vnzBFBbU1qHIJ8tSEQEdlEzhSCdStS4xD00zgEIiKbyJnbR5e0lrA4cSCHbz866CgiIr1KzhSCMQdO5O1h+1NWXrn1lUVEckjOFIKRA0oYOaAk6BgiIr1OzlwjEBGRzqkQiIh08EW6oQa48cYbaWradsY9USEQEekg1wpBzlwjEJFt2J3HbD5v9xNg33OhrQnu+8bmy8d+C8adDutXw4NnbbrsnL9+5st17Ib6hhtu4IYbbuDBBx+ktbWVE088kV/84hesX7+eU045hZqaGhKJBJdffjkrV65k2bJlHH744VRVVTF9+vRNtn311Vfz5JNP0tzczIEHHsgtt9yCmbFgwQK+//3vU1tbSzgc5qGHHmLkyJFcd9113HvvvYRCIY466iiuvfbaLr55W6dCICLSQcduqJ999lnmz5/Pa6+9hrtz/PHHM2PGDGpraxkyZAh//WuqsDQ0NFBWVsavf/1rpk+fvkmXERtMmTKFK664AoAzzzyTp556iuOOO47TTz+dqVOncuKJJ9LS0kIymeTpp5/m8ccf59VXX6WoqKjb+hbqSIVARHq/z/oEHy367OXFlVs9AtiaZ599lmeffZZx48YB0NjYyPz58znkkEO46KKLuOSSSzj22GM55JBDtrqt6dOnc/3119PU1MSaNWvYfffdOeyww1i6dCknnngiAAUFBUCqK+pzzjmHoqIioPu6ne5IhUBEZCvcnUsvvZTzzjtvs2WzZ89m2rRpXHbZZRx55JEbP+13pqWlhfPPP59Zs2YxbNgwrrrqqkC6ne5IF4tFRDro2A311772Ne644w4aGxsBWLp0KatWrWLZsmUUFRVxxhlncPHFFzN79uxOn7/Bhp1+VVUVjY2NPPzwwxvXr66u5rHHHgOgtbWVpqYmJkyYwJ133rnxwrNODYmI9JD23VAfddRR3HDDDcybN48DDjgAgJKSEu69914WLFjAxRdfTCgUIhKJ8Pvf/x6AyZMnM3HiRIYMGbLJxeLy8nLOPfdcxowZw+DBg9lnn302Lrvnnns477zzuOKKK4hEIjz00ENMnDiRt956i/HjxxONRjn66KP5z//8z27/eXOmG2oR2XaoG+ovRt1Qi4hIl6gQiIjkOBUCEemVtrXT1r3F53nfVAhEpNcpKChg9erVKgZd5O6sXr16YzuETOmuIRHpdaqrq6mpqaG2tjboKNucgoICqquru/QcFQIR6XUikQgjRmh88Z6S1VNDZjbRzD4wswVmNrWT5flm9uf08lfNbHg284iIyOayVgjMLAzcDBwF7AacZma7dVjtu0Cdu+8E/Aa4Llt5RESkc9k8ItgXWODuC929DXgAmNRhnUnA3enHDwNHmpllMZOIiHSQzWsEQ4El7aZrgP22tI67x82sAagEPm2/kplNBianJxvN7IPPmamq47Z7CeXqGuXqut6aTbm65ovk2mFLC7aJi8Xufitw6xfdjpnN2lIT6yApV9coV9f11mzK1TXZypXNU0NLgWHtpqvT8zpdx8zygDJgdRYziYhIB9ksBK8Do8xshJlFgW8CT3RY5wng7PTjrwN/d7UgERHpUVk7NZQ+5z8F+BsQBu5w9zlmdjUwy92fAP4I3GNmC4A1pIpFNn3h00tZolxdo1xd11uzKVfXZCXXNtcNtYiIdC/1NSQikuNUCEREclzOFIKtdXcRBDMbZmbTzWyumc0xswuDztSemYXN7E0zeyroLBuYWbmZPWxm75vZPDM7IOhMAGb24/Tv8D0zu9/Mutb9Y/fluMPMVpnZe+3m9Tez58xsfvp7RS/JdUP69/iOmT1qZuW9IVe7ZReZmZtZVW/JZWYXpN+zOWZ2fXe9Xk4Uggy7uwhCHLjI3XcD9gd+0EtybXAhMC/oEB38FnjG3XcB9qQX5DOzocAPgfHuPobUzRHZvvFhS+4CJnaYNxV4wd1HAS+kp3vaXWye6zlgjLvvAXwIXNrToeg8F2Y2DPgqsLinA6XdRYdcZnY4qd4Y9nT33YFfddeL5UQhILPuLnqcuy9399npx+tI7dSGBpsqxcyqgWOA24POsoGZlQFfJnW3Ge7e5u71gYb6//KAwnR7mCJgWRAh3H0GqTvw2mvflcvdwAk9mQk6z+Xuz7p7PD05k1Rbo8Bzpf0G+HcgkLtptpDr34Br3b01vc6q7nq9XCkEnXV30St2uBuke14dB7wacJQNbiT1j5AMOEd7I4Ba4M70Kavbzaw46FDuvpTUp7PFwHKgwd2fDTbVJga5+/L04xXAoCDDbMF3gKeDDgFgZpOApe7+dtBZOhgNHJLuqfklM9unuzacK4WgVzOzEuAR4EfuvrYX5DkWWOXubwSdpYM8YC/g9+4+DlhPMKc5NpE+5z6JVKEaAhSb2RnBpupcusFmr7pn3Mx+Tuo06X29IEsR8DPgiqCzdCIP6E/qNPLFwIPd1UlnrhSCTLq7CISZRUgVgfvc/S9B50k7CDjezBaROo12hJndG2wkIHUkV+PuG46aHiZVGIL2FeBjd6919xjwF+DAgDO1t9LMtgNIf++2UwpflJl9GzgWOL2X9CowklRBfzv9918NzDazwYGmSqkB/uIpr5E6Wu+WC9m5Uggy6e6ix6Wr+R+Bee7+66DzbODul7p7tbsPJ/Ve/d3dA/+E6+4rgCVmtnN61pHA3AAjbbAY2N/MitK/0yPpBRex22nflcvZwOMBZtnIzCaSOv14vLs3BZ0HwN3fdfeB7j48/fdfA+yV/tsL2mPA4QBmNhqI0k09pOZEIUhfkNrQ3cU84EF3nxNsKiD1yftMUp+430p/HR10qF7uAuA+M3sHGAv8Z7BxIH2E8jAwG3iX1P9VIF0UmNn9wCvAzmZWY2bfBa4FJpjZfFJHL9f2klz/C5QCz6X/9v/QS3IFbgu57gB2TN9S+gBwdncdRamLCRGRHJcTRwQiIrJlKgQiIjlOhUBEJMepEIiI5DgVAhGRHKdCIJJlZnZYb+rBVaQjFQIRkRynQiCSZmZnmNlr6cZNt6THY2g0s9+k+39/wcwGpNcda2Yz2/WlX5Gev5OZPW9mb5vZbDMbmd58SbtxFO7b0EeMmV1rqfEo3jGzbutWWKQrVAhEADPbFTgVOMjdxwIJ4HSgGJiV7v/9JeDK9FP+BFyS7kv/3Xbz7wNudvc9SfU3tKHXz3HAj0iNh7EjcJCZVQInArunt/PLbP6MIluiQiCSciSwN/C6mb2Vnt6RVMdef06vcy9wcHpchHJ3fyk9/27gy2ZWCgx190cB3L2lXR86r7l7jbsngbeA4UAD0AL80cxOAnpFfzuSe1QIRFIMuNvdx6a/dnb3qzpZ7/P2ydLa7nECyEv3gbUvqX6KjgWe+ZzbFvlCVAhEUl4Avm5mA2HjOL87kPof+Xp6nW8BL7t7A1BnZoek558JvJQeZa7GzE5IbyM/3b99p9LjUJS5+zTgx6SG3hTpcXlBBxDpDdx9rpldBjxrZiEgBvyA1OA3+6aXrSJ1HQFS3Tn/Ib2jXwick55/JnCLmV2d3sY3PuNlS4HHLTXQvQE/6eYfSyQj6n1U5DOYWaO7lwSdQySbdGpIRCTH6YhARCTH6YhARCTHqRCIiOQ4FQIRkRynQiAikuNUCEREctz/A521W9ddouXjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
