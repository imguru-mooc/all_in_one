{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5*np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4173"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.1, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "sum_squares_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2123"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.8, 0.0, 0.11, 0.0, 0.0, 0.6, 0.0, 0.0])\n",
    "sum_squares_error(y, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크로스 엔트로피 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "delta = 1e-7\n",
    "x = np.arange(0.0+delta, 1.1, 0.01)\n",
    "y = -np.log(x)\n",
    "plt.ylim(0,5)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum( t*np.log(y + delta) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.1, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010050234843405595"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.99, 0.0, 0.11, 0.0, 0.0, 0.6, 0.0, 0.0])\n",
    "cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "print(batch_mask)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    if y.ndim == 1 :\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum( t*np.log(y + delta) ) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수치 미분 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-50\n"
     ]
    }
   ],
   "source": [
    "h = 1e-50\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-50 # 0.000000000000000000000000000000000000000000000000001\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-04"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-4) # 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 \n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n"
     ]
    }
   ],
   "source": [
    "def function(x):\n",
    "    return 0.01*x**2 + 0.1*x  # 0.02*5 + 0.1\n",
    "\n",
    "# 0.20000099999917254\n",
    "# 0.1999999999990898\n",
    "print(numerical_diff(function, 5))\n",
    "# print(numerical_diff(function, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x \n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_diff(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "     \n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tf = tangent_line(function_1, 7)\n",
    "y2 = tf(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 편미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0**2 + 4**2\n",
    "\n",
    "numerical_diff(function_tmp1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3**2 + x1**2\n",
    "\n",
    "numerical_diff(function_tmp2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기(수치미분 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2 \n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1)\n",
    "\n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_gradient(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "     \n",
    "if __name__ == '__main__':\n",
    "    x0 = np.arange(-2, 2.5, 0.25)\n",
    "    x1 = np.arange(-2, 2.5, 0.25)\n",
    "    X, Y = np.meshgrid(x0, x1)\n",
    "    \n",
    "    X = X.flatten()\n",
    "    Y = Y.flatten()\n",
    "    \n",
    "    grad = numerical_gradient(function_2, np.array([X, Y]) )\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")#,headwidth=10,scale=40,color=\"#444444\")\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel('x0')\n",
    "    plt.ylabel('x1')\n",
    "    plt.grid()\n",
    "#     plt.legend()\n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from gradient_2d import numerical_gradient\n",
    "\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([3.0, -4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W) # (3,)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x) # (3,)\n",
    "#         print(z.shape)\n",
    "        y = softmax(z)      # (3,)\n",
    "#         print(y.shape)\n",
    "        loss = cross_entropy_error(y, t) # 스칼라 값\n",
    "#         print(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9]) # (2,)\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "#     grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
