{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  np.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4])\n",
    "print(a.shape)\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4])\n",
    "print(a.shape)\n",
    "print(a.reshape(4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### axis=0, axis=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3]\n",
      " [4 5 6 7]]\n",
      "[ 6 22]\n",
      "[ 4  6  8 10]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(8).reshape(2,4)\n",
    "print(a)\n",
    "print(np.sum(a, axis=1))\n",
    "print(np.sum(a, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayer:    \n",
    "    def __init__(self, learning_rate=0.1, l1=0, l2=0):\n",
    "        self.w = None              # 가중치\n",
    "        self.b = None              # 절편\n",
    "        self.losses = []           # 훈련 손실\n",
    "        self.val_losses = []       # 검증 손실\n",
    "        self.w_history = []        # 가중치 기록\n",
    "        self.lr = learning_rate    # 학습률\n",
    "        self.l1 = l1               # L1 손실 하이퍼파라미터\n",
    "        self.l2 = l2               # L2 손실 하이퍼파라미터\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z = np.dot(x, self.w) + self.b        # 선형 출력을 계산합니다.\n",
    "        return z\n",
    "\n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)\n",
    "        w_grad = np.dot(x.T, err) / m         # 가중치에 대한 그래디언트를 계산합니다.\n",
    "        b_grad = np.sum(err) / m              # 절편에 대한 그래디언트를 계산합니다.\n",
    "        return w_grad, b_grad\n",
    "\n",
    "    def activation(self, z):\n",
    "        z = np.clip(z, -100, None)            # 안전한 np.exp() 계산을 위해\n",
    "        a = 1 / (1 + np.exp(-z))              # 시그모이드 계산\n",
    "        return a\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y = y.reshape(-1, 1)                  # 타깃을 열 벡터로 바꿉니다.\n",
    "        y_val = y_val.reshape(-1, 1)\n",
    "        m = len(x)                            # 샘플 개수를 저장합니다.\n",
    "        self.w = np.ones((x.shape[1], 1))     # 가중치를 초기화합니다.\n",
    "        self.b = 0                            # 절편을 초기화합니다.\n",
    "        self.w_history.append(self.w.copy())  # 가중치를 기록합니다.\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            z = self.forpass(x)               # 정방향 계산을 수행합니다.\n",
    "            a = self.activation(z)            # 활성화 함수를 적용합니다.\n",
    "            err = a - y                   # 오차를 계산합니다.\n",
    "            # 오차를 역전파하여 그래디언트를 계산합니다.\n",
    "            w_grad, b_grad = self.backprop(x, err)\n",
    "            # 그래디언트에서 페널티 항의 미분 값을 더합니다.\n",
    "            w_grad += (self.l1 * np.sign(self.w) + self.l2 * self.w) / m\n",
    "            # 가중치와 절편을 업데이트합니다.\n",
    "            self.w -= self.lr * w_grad\n",
    "            self.b -= self.lr * b_grad\n",
    "            # 가중치를 기록합니다.\n",
    "            self.w_history.append(self.w.copy())\n",
    "            # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "            loss = np.sum(-(y*np.log(a) + (1-y)*np.log(1-a)))\n",
    "            self.losses.append((loss + self.reg_loss()) / m)\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)      # 정방향 계산을 수행합니다.\n",
    "        return z > 0             # 스텝 함수를 적용합니다.\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환합니다.\n",
    "        return np.mean(self.predict(x) == y.reshape(-1, 1))\n",
    "    \n",
    "    def reg_loss(self):\n",
    "        # 가중치에 규제를 적용합니다.\n",
    "        return self.l1 * np.sum(np.abs(self.w)) + self.l2 / 2 * np.sum(self.w**2)\n",
    "    \n",
    "    def update_val_loss(self, x_val, y_val):\n",
    "        z = self.forpass(x_val)            # 정방향 계산을 수행합니다.\n",
    "        a = self.activation(z)             # 활성화 함수를 적용합니다.\n",
    "        a = np.clip(a, 1e-10, 1-1e-10)     # 출력 값을 클리핑합니다.\n",
    "        # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "        val_loss = np.sum(-(y_val*np.log(a) + (1-y_val)*np.log(1-a)))\n",
    "        self.val_losses.append((val_loss + self.reg_loss()) / len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualLayer(SingleLayer):\n",
    "    \n",
    "    def __init__(self, units=10, learning_rate=0.1, l1=0, l2=0):\n",
    "        self.units = units         # 은닉층의 뉴런 개수\n",
    "        self.w1 = None             # 은닉층의 가중치\n",
    "        self.b1 = None             # 은닉층의 절편\n",
    "        self.w2 = None             # 출력층의 가중치\n",
    "        self.b2 = None             # 출력층의 절편\n",
    "        self.a1 = None             # 은닉층의 활성화 출력\n",
    "        self.losses = []           # 훈련 손실\n",
    "        self.val_losses = []       # 검증 손실\n",
    "        self.lr = learning_rate    # 학습률\n",
    "        self.l1 = l1               # L1 손실 하이퍼파라미터\n",
    "        self.l2 = l2               # L2 손실 하이퍼파라미터\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z1 = np.dot(x, self.w1) + self.b1        # 첫 번째 층의 선형 식을 계산합니다\n",
    "        self.a1 = self.activation(z1)            # 활성화 함수를 적용합니다\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2  # 두 번째 층의 선형 식을 계산합니다.\n",
    "        return z2\n",
    "\n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)       # 샘플 개수\n",
    "        # 출력층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w2_grad = np.dot(self.a1.T, err) / m\n",
    "        b2_grad = np.sum(err, axis=0) / m\n",
    "        # 시그모이드 함수까지 그래디언트를 계산합니다.\n",
    "        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)\n",
    "        # 은닉층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w1_grad = np.dot(x.T, err_to_hidden) / m\n",
    "        b1_grad = np.sum(err_to_hidden, axis=0) / m\n",
    "        return w1_grad, b1_grad, w2_grad, b2_grad\n",
    "\n",
    "    def init_weights(self, n_features):\n",
    "        self.w1 = np.ones((n_features, self.units))  # (특성 개수, 은닉층의 크기)\n",
    "        self.b1 = np.zeros(self.units)               # 은닉층의 크기\n",
    "        self.w2 = np.ones((self.units, 1))           # (은닉층의 크기, 1)\n",
    "        self.b2 = 0\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y = y.reshape(-1, 1)          # 타깃을 열 벡터로 바꿉니다.\n",
    "        y_val = y_val.reshape(-1, 1)\n",
    "        m = len(x)                    # 샘플 개수를 저장합니다.\n",
    "        self.init_weights(x.shape[1]) # 은닉층과 출력층의 가중치를 초기화합니다.\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            a = self.training(x, y, m)\n",
    "            # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "            loss = np.sum(-(y*np.log(a) + (1-y)*np.log(1-a)))\n",
    "            self.losses.append((loss + self.reg_loss()) / m)\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "            \n",
    "    def training(self, x, y, m):\n",
    "        z = self.forpass(x)       # 정방향 계산을 수행합니다.\n",
    "        a = self.activation(z)    # 활성화 함수를 적용합니다.\n",
    "        err = (a - y)            # 오차를 계산합니다.\n",
    "        # 오차를 역전파하여 그래디언트를 계산합니다.\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        # 그래디언트에서 페널티 항의 미분 값을 뺍니다\n",
    "        w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m\n",
    "        w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m\n",
    "        # 은닉층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w1 -= self.lr * w1_grad\n",
    "        self.b1 -= self.lr * b1_grad\n",
    "        # 출력층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w2 -= self.lr * w2_grad\n",
    "        self.b2 -= self.lr * b2_grad\n",
    "        return a\n",
    "    \n",
    "    def reg_loss(self):\n",
    "        # 은닉층과 출력층의 가중치에 규제를 적용합니다.\n",
    "        return self.l1 * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + \\\n",
    "               self.l2 / 2 * (np.sum(self.w1**2) + np.sum(self.w2**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomInitNetwork(DualLayer):\n",
    "    \n",
    "    def init_weights(self, n_features):\n",
    "        np.random.seed(42)\n",
    "        self.w1 = np.random.normal(0, 1, \n",
    "                                   (n_features, self.units))  # (특성 개수, 은닉층의 크기)\n",
    "        self.b1 = np.zeros(self.units)                        # 은닉층의 크기\n",
    "        self.w2 = np.random.normal(0, 1, \n",
    "                                   (self.units, 1))           # (은닉층의 크기, 1)\n",
    "        self.b2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchNetwork(RandomInitNetwork):\n",
    "    \n",
    "    def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n",
    "        super().__init__(units, learning_rate, l1, l2)\n",
    "        self.batch_size = batch_size     # 배치 크기\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y_val = y_val.reshape(-1, 1)     # 타깃을 열 벡터로 바꿉니다.\n",
    "        self.init_weights(x.shape[1])    # 은닉층과 출력층의 가중치를 초기화합니다.\n",
    "        np.random.seed(42)\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            # 제너레이터 함수에서 반환한 미니배치를 순환합니다.\n",
    "            for x_batch, y_batch in self.gen_batch(x, y):\n",
    "                y_batch = y_batch.reshape(-1, 1) # 타깃을 열 벡터로 바꿉니다.\n",
    "                m = len(x_batch)                 # 샘플 개수를 저장합니다.\n",
    "                a = self.training(x_batch, y_batch, m)\n",
    "                # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "                loss += np.sum(-(y_batch*np.log(a) + (1-y_batch)*np.log(1-a)))\n",
    "            self.losses.append((loss + self.reg_loss()) / len(x))\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "\n",
    "    # 미니배치 제너레이터 함수\n",
    "    def gen_batch(self, x, y):\n",
    "        length = len(x)\n",
    "        bins = length // self.batch_size # 미니배치 횟수\n",
    "        if length % self.batch_size:\n",
    "            bins += 1                    # 나누어 떨어지지 않을 때\n",
    "        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞습니다.\n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size * i\n",
    "            end = self.batch_size * (i + 1)\n",
    "            yield x[start:end], y[start:end]   # batch_size만큼 슬라이싱하여 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassNetwork(MinibatchNetwork):\n",
    "    def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n",
    "        self.units = units         # 은닉층의 뉴런 개수\n",
    "        self.batch_size = batch_size     # 배치 크기\n",
    "        self.w1 = None             # 은닉층의 가중치\n",
    "        self.b1 = None             # 은닉층의 절편\n",
    "        self.w2 = None             # 출력층의 가중치\n",
    "        self.b2 = None             # 출력층의 절편\n",
    "        self.a1 = None             # 은닉층의 활성화 출력\n",
    "        self.losses = []           # 훈련 손실\n",
    "        self.val_losses = []       # 검증 손실\n",
    "        self.lr = learning_rate    # 학습률\n",
    "        self.l1 = l1               # L1 손실 하이퍼파라미터\n",
    "        self.l2 = l2               # L2 손실 하이퍼파라미터\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z1 = np.dot(x, self.w1) + self.b1        # 첫 번째 층의 선형 식을 계산합니다\n",
    "        self.a1 = self.sigmoid(z1)               # 활성화 함수를 적용합니다\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2  # 두 번째 층의 선형 식을 계산합니다.\n",
    "        return z2\n",
    "\n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)       # 샘플 개수\n",
    "        # 출력층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w2_grad = np.dot(self.a1.T, err) / m\n",
    "        b2_grad = np.sum(err) / m\n",
    "        # 시그모이드 함수까지 그래디언트를 계산합니다.\n",
    "        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)\n",
    "        # 은닉층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w1_grad = np.dot(x.T, err_to_hidden) / m\n",
    "        b1_grad = np.sum(err_to_hidden, axis=0) / m\n",
    "        return w1_grad, b1_grad, w2_grad, b2_grad\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -100, None)            # 안전한 np.exp() 계산을 위해\n",
    "        a = 1 / (1 + np.exp(-z))              # 시그모이드 계산\n",
    "        return a\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        # 소프트맥스 함수\n",
    "        z = np.clip(z, -100, None)            # 안전한 np.exp() 계산을 위해\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1).reshape(-1, 1)\n",
    " \n",
    "    def init_weights(self, n_features, n_classes):\n",
    "        self.w1 = np.random.normal(0, 1, \n",
    "                                   (n_features, self.units))  # (특성 개수, 은닉층의 크기)\n",
    "        self.b1 = np.zeros(self.units)                        # 은닉층의 크기\n",
    "        self.w2 = np.random.normal(0, 1, \n",
    "                                   (self.units, n_classes))   # (은닉층의 크기, 클래스 개수)\n",
    "        self.b2 = np.zeros(n_classes)\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        np.random.seed(42)\n",
    "        self.init_weights(x.shape[1], y.shape[1])    # 은닉층과 출력층의 가중치를 초기화합니다.\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            print('.', end='')\n",
    "            # 제너레이터 함수에서 반환한 미니배치를 순환합니다.\n",
    "            for x_batch, y_batch in self.gen_batch(x, y):\n",
    "                a = self.training(x_batch, y_batch)\n",
    "                # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "                loss += np.sum(-y_batch*np.log(a))\n",
    "            self.losses.append((loss + self.reg_loss()) / len(x))\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "\n",
    "    # 미니배치 제너레이터 함수\n",
    "    def gen_batch(self, x, y):\n",
    "        length = len(x)\n",
    "        bins = length // self.batch_size # 미니배치 횟수\n",
    "        if length % self.batch_size:\n",
    "            bins += 1                    # 나누어 떨어지지 않을 때\n",
    "        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞습니다.\n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size * i\n",
    "            end = self.batch_size * (i + 1)\n",
    "            yield x[start:end], y[start:end]   # batch_size만큼 슬라이싱하여 반환합니다.\n",
    "            \n",
    "    def training(self, x, y):\n",
    "        m = len(x)                # 샘플 개수를 저장합니다.\n",
    "        z = self.forpass(x)       # 정방향 계산을 수행합니다.\n",
    "        a = self.softmax(z)       # 활성화 함수를 적용합니다.\n",
    "        err = (a - y)            # 오차를 계산합니다.\n",
    "        # 오차를 역전파하여 그래디언트를 계산합니다.\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        # 그래디언트에서 페널티 항의 미분 값을 뺍니다\n",
    "        w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m\n",
    "        w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m\n",
    "        # 은닉층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w1 -= self.lr * w1_grad\n",
    "        self.b1 -= self.lr * b1_grad\n",
    "        # 출력층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w2 -= self.lr * w2_grad\n",
    "        self.b2 -= self.lr * b2_grad\n",
    "        return a\n",
    "   \n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)          # 정방향 계산을 수행합니다.\n",
    "        return np.argmax(z, axis=1)  # 가장 큰 값의 인덱스를 반환합니다.\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환합니다.\n",
    "        return np.mean(self.predict(x) == np.argmax(y, axis=1))\n",
    "\n",
    "    def reg_loss(self):\n",
    "        # 은닉층과 출력층의 가중치에 규제를 적용합니다.\n",
    "        return self.l1 * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + \\\n",
    "               self.l2 / 2 * (np.sum(self.w1**2) + np.sum(self.w2**2))\n",
    "\n",
    "    def update_val_loss(self, x_val, y_val):\n",
    "        z = self.forpass(x_val)            # 정방향 계산을 수행합니다.\n",
    "        a = self.softmax(z)                # 활성화 함수를 적용합니다.\n",
    "        a = np.clip(a, 1e-10, 1-1e-10)     # 출력 값을 클리핑합니다.\n",
    "        # 크로스 엔트로피 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "        val_loss = np.sum(-y_val*np.log(a))\n",
    "        self.val_losses.append((val_loss + self.reg_loss()) / len(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_all.shape, y_train_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train_all[3], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4 3 1 4 8 4 3 0 2 4 4 5 3 6 6 0 8 5\n",
      " 2 1 6 6 7 9 5 9 2 7 3 0 3 3 3 7 2 2 6 6 8 3 3 5 0 5 5 0 2 0 0 4 1 3 1 6 3\n",
      " 1 4 4 6 1 9 1 3 5 7 9 7 1 7 9 9 9 3 2 9 3 6 4 1 1 8]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_all[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "드레스\n"
     ]
    }
   ],
   "source": [
    "class_names = ['티셔츠/윗도리', '바지', '스웨터', '드레스', '코트', \n",
    "               '샌들', '셔츠', '스니커즈', '가방', '앵클부츠']\n",
    "print(class_names[y_train_all[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### np.bincount 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n",
      "[2 1 1 4 3 1]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0,4,4,1,2,3,3,3,3,4,5,0])\n",
    "print(len(a))\n",
    "print(a.shape[0])\n",
    "print(np.bincount(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_train_all))\n",
    "np.bincount(y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_test))\n",
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_val))\n",
    "np.bincount(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   1   1   0   0   0  25  55   0   0   0   0   0   0  43\n",
      "   24   0   0   0   1   0   0   0   0   0]\n",
      " [  0   0   0   1   1   1   0  25 147 189 181  96  51  49  55  37  76 217\n",
      "  172 125  20   0   2   1   0   0   0   0]\n",
      " [  0   0   0   1   3   0  82 206 202 190 178 188 239 152  99 199 201 162\n",
      "  176 192 185  71   0   2   0   0   0   0]\n",
      " [  0   0   0   1   0   0 202 196 181 188 183 169 179 160 107 198 166 167\n",
      "  178 170 183 166   0   0   1   0   0   0]\n",
      " [  0   0   0   1   0  48 216 184 185 185 190 175 164 188 198 188 157 183\n",
      "  176 171 167 175  26   0   2   0   0   0]\n",
      " [  0   0   0   0   0  90 224 183 185 184 183 190 164 169 193 158 170 174\n",
      "  176 175 165 180  62   0   0   0   0   0]\n",
      " [  0   0   0   0   0 123 228 184 180 183 178 183 172 172 172 161 179 170\n",
      "  174 175 165 181 106   0   0   0   0   0]\n",
      " [  0   0   0   0   0 156 229 188 175 178 175 175 190 157 157 180 169 169\n",
      "  172 171 161 179 143   0   0   0   0   0]\n",
      " [  0   0   0   0   0 181 228 190 174 174 176 175 197 167 167 183 166 169\n",
      "  171 170 162 175 172   0   0   0   0   0]\n",
      " [  0   0   0   0   6 175 221 192 170 174 176 175 193 176 172 175 167 169\n",
      "  169 167 160 169 196   0   0   0   0   0]\n",
      " [  0   0   0   0  23 184 216 206 164 175 176 175 192 178 175 174 167 166\n",
      "  169 169 165 169 175   6   0   0   0   0]\n",
      " [  0   0   0   0  53 189 208 219 161 176 176 175 190 181 178 172 169 165\n",
      "  170 169 166 167 183  26   0   0   0   0]\n",
      " [  0   0   0   0  84 194 207 216 157 176 175 176 193 183 180 172 167 167\n",
      "  166 170 165 167 185  61   0   0   0   0]\n",
      " [  0   0   0   0 114 193 211 207 157 175 175 176 196 183 181 172 166 166\n",
      "  165 170 164 169 187  88   0   0   0   0]\n",
      " [  0   0   0   0 155 192 219 194 161 172 178 176 197 185 181 169 166 165\n",
      "  165 171 164 171 184 108   0   0   0   0]\n",
      " [  0   0   0   0 181 192 225 185 162 172 179 176 199 185 183 169 166 164\n",
      "  164 172 161 170 184 135   0   0   0   0]\n",
      " [  0   0   0   0 210 189 233 180 161 171 179 179 201 185 190 166 166 164\n",
      "  164 176 158 171 183 155   0   0   0   0]\n",
      " [  0   0   0   0 229 184 234 176 164 169 180 183 198 185 193 166 167 164\n",
      "  165 179 157 176 179 174   0   0   0   0]\n",
      " [  0   0   0   2 237 183 237 171 166 167 178 188 196 187 196 164 166 165\n",
      "  166 179 157 178 176 185   2   0   0   0]\n",
      " [  0   0   0  24 246 184 255 172 166 169 180 192 196 189 201 161 164 165\n",
      "  165 181 161 181 176 189  17   0   0   0]\n",
      " [  0   0   0  52 247 187 238 166 170 170 180 198 193 189 207 160 165 165\n",
      "  164 180 162 187 175 192  52   0   0   0]\n",
      " [  0   0   0  82 215 192 239 158 169 174 179 207 189 187 213 160 167 166\n",
      "  165 181 169 190 170 192  73   0   0   0]\n",
      " [  0   0   0 102 212 201 212 166 169 171 179 217 189 187 217 157 170 167\n",
      "  166 178 171 196 169 171  84   0   0   0]\n",
      " [  0   0   0 166 211 213 193 164 170 170 180 226 181 187 228 157 167 167\n",
      "  158 188 196 128 170 179 128   0   0   0]\n",
      " [  0   0   0  35  92 129 199 184 171 179 187 246 170 189 240 156 170 174\n",
      "  166 190 230 139 102 139  89   0   0   0]\n",
      " [  0   0   0   0   0   0 142 213 166 151 178 235 190 162 244 179 183 160\n",
      "  142 164 208 125   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1   2   0  96 181 179 178 178 185 160 208 171 167 167\n",
      "  184 174 105   0   1   2   1   0   0   0]\n",
      " [  0   0   0   0   0   0   2   0   1  52  83  98 112 134 146 105  89  73\n",
      "   49   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_val = x_val / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.00392157 0.00392157\n",
      "  0.         0.         0.         0.09803922 0.21568627 0.\n",
      "  0.         0.         0.         0.         0.         0.16862745\n",
      "  0.09411765 0.         0.         0.         0.00392157 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00392157 0.00392157 0.00392157\n",
      "  0.         0.09803922 0.57647059 0.74117647 0.70980392 0.37647059\n",
      "  0.2        0.19215686 0.21568627 0.14509804 0.29803922 0.85098039\n",
      "  0.6745098  0.49019608 0.07843137 0.         0.00784314 0.00392157\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00392157 0.01176471 0.\n",
      "  0.32156863 0.80784314 0.79215686 0.74509804 0.69803922 0.7372549\n",
      "  0.9372549  0.59607843 0.38823529 0.78039216 0.78823529 0.63529412\n",
      "  0.69019608 0.75294118 0.7254902  0.27843137 0.         0.00784314\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00392157 0.         0.\n",
      "  0.79215686 0.76862745 0.70980392 0.7372549  0.71764706 0.6627451\n",
      "  0.70196078 0.62745098 0.41960784 0.77647059 0.65098039 0.65490196\n",
      "  0.69803922 0.66666667 0.71764706 0.65098039 0.         0.\n",
      "  0.00392157 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00392157 0.         0.18823529\n",
      "  0.84705882 0.72156863 0.7254902  0.7254902  0.74509804 0.68627451\n",
      "  0.64313725 0.7372549  0.77647059 0.7372549  0.61568627 0.71764706\n",
      "  0.69019608 0.67058824 0.65490196 0.68627451 0.10196078 0.\n",
      "  0.00784314 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.35294118\n",
      "  0.87843137 0.71764706 0.7254902  0.72156863 0.71764706 0.74509804\n",
      "  0.64313725 0.6627451  0.75686275 0.61960784 0.66666667 0.68235294\n",
      "  0.69019608 0.68627451 0.64705882 0.70588235 0.24313725 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.48235294\n",
      "  0.89411765 0.72156863 0.70588235 0.71764706 0.69803922 0.71764706\n",
      "  0.6745098  0.6745098  0.6745098  0.63137255 0.70196078 0.66666667\n",
      "  0.68235294 0.68627451 0.64705882 0.70980392 0.41568627 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.61176471\n",
      "  0.89803922 0.7372549  0.68627451 0.69803922 0.68627451 0.68627451\n",
      "  0.74509804 0.61568627 0.61568627 0.70588235 0.6627451  0.6627451\n",
      "  0.6745098  0.67058824 0.63137255 0.70196078 0.56078431 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.70980392\n",
      "  0.89411765 0.74509804 0.68235294 0.68235294 0.69019608 0.68627451\n",
      "  0.77254902 0.65490196 0.65490196 0.71764706 0.65098039 0.6627451\n",
      "  0.67058824 0.66666667 0.63529412 0.68627451 0.6745098  0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.02352941 0.68627451\n",
      "  0.86666667 0.75294118 0.66666667 0.68235294 0.69019608 0.68627451\n",
      "  0.75686275 0.69019608 0.6745098  0.68627451 0.65490196 0.6627451\n",
      "  0.6627451  0.65490196 0.62745098 0.6627451  0.76862745 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.09019608 0.72156863\n",
      "  0.84705882 0.80784314 0.64313725 0.68627451 0.69019608 0.68627451\n",
      "  0.75294118 0.69803922 0.68627451 0.68235294 0.65490196 0.65098039\n",
      "  0.6627451  0.6627451  0.64705882 0.6627451  0.68627451 0.02352941\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.20784314 0.74117647\n",
      "  0.81568627 0.85882353 0.63137255 0.69019608 0.69019608 0.68627451\n",
      "  0.74509804 0.70980392 0.69803922 0.6745098  0.6627451  0.64705882\n",
      "  0.66666667 0.6627451  0.65098039 0.65490196 0.71764706 0.10196078\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.32941176 0.76078431\n",
      "  0.81176471 0.84705882 0.61568627 0.69019608 0.68627451 0.69019608\n",
      "  0.75686275 0.71764706 0.70588235 0.6745098  0.65490196 0.65490196\n",
      "  0.65098039 0.66666667 0.64705882 0.65490196 0.7254902  0.23921569\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.44705882 0.75686275\n",
      "  0.82745098 0.81176471 0.61568627 0.68627451 0.68627451 0.69019608\n",
      "  0.76862745 0.71764706 0.70980392 0.6745098  0.65098039 0.65098039\n",
      "  0.64705882 0.66666667 0.64313725 0.6627451  0.73333333 0.34509804\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.60784314 0.75294118\n",
      "  0.85882353 0.76078431 0.63137255 0.6745098  0.69803922 0.69019608\n",
      "  0.77254902 0.7254902  0.70980392 0.6627451  0.65098039 0.64705882\n",
      "  0.64705882 0.67058824 0.64313725 0.67058824 0.72156863 0.42352941\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.70980392 0.75294118\n",
      "  0.88235294 0.7254902  0.63529412 0.6745098  0.70196078 0.69019608\n",
      "  0.78039216 0.7254902  0.71764706 0.6627451  0.65098039 0.64313725\n",
      "  0.64313725 0.6745098  0.63137255 0.66666667 0.72156863 0.52941176\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.82352941 0.74117647\n",
      "  0.91372549 0.70588235 0.63137255 0.67058824 0.70196078 0.70196078\n",
      "  0.78823529 0.7254902  0.74509804 0.65098039 0.65098039 0.64313725\n",
      "  0.64313725 0.69019608 0.61960784 0.67058824 0.71764706 0.60784314\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.89803922 0.72156863\n",
      "  0.91764706 0.69019608 0.64313725 0.6627451  0.70588235 0.71764706\n",
      "  0.77647059 0.7254902  0.75686275 0.65098039 0.65490196 0.64313725\n",
      "  0.64705882 0.70196078 0.61568627 0.69019608 0.70196078 0.68235294\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00784314 0.92941176 0.71764706\n",
      "  0.92941176 0.67058824 0.65098039 0.65490196 0.69803922 0.7372549\n",
      "  0.76862745 0.73333333 0.76862745 0.64313725 0.65098039 0.64705882\n",
      "  0.65098039 0.70196078 0.61568627 0.69803922 0.69019608 0.7254902\n",
      "  0.00784314 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.09411765 0.96470588 0.72156863\n",
      "  1.         0.6745098  0.65098039 0.6627451  0.70588235 0.75294118\n",
      "  0.76862745 0.74117647 0.78823529 0.63137255 0.64313725 0.64705882\n",
      "  0.64705882 0.70980392 0.63137255 0.70980392 0.69019608 0.74117647\n",
      "  0.06666667 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.20392157 0.96862745 0.73333333\n",
      "  0.93333333 0.65098039 0.66666667 0.66666667 0.70588235 0.77647059\n",
      "  0.75686275 0.74117647 0.81176471 0.62745098 0.64705882 0.64705882\n",
      "  0.64313725 0.70588235 0.63529412 0.73333333 0.68627451 0.75294118\n",
      "  0.20392157 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.32156863 0.84313725 0.75294118\n",
      "  0.9372549  0.61960784 0.6627451  0.68235294 0.70196078 0.81176471\n",
      "  0.74117647 0.73333333 0.83529412 0.62745098 0.65490196 0.65098039\n",
      "  0.64705882 0.70980392 0.6627451  0.74509804 0.66666667 0.75294118\n",
      "  0.28627451 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.4        0.83137255 0.78823529\n",
      "  0.83137255 0.65098039 0.6627451  0.67058824 0.70196078 0.85098039\n",
      "  0.74117647 0.73333333 0.85098039 0.61568627 0.66666667 0.65490196\n",
      "  0.65098039 0.69803922 0.67058824 0.76862745 0.6627451  0.67058824\n",
      "  0.32941176 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.65098039 0.82745098 0.83529412\n",
      "  0.75686275 0.64313725 0.66666667 0.66666667 0.70588235 0.88627451\n",
      "  0.70980392 0.73333333 0.89411765 0.61568627 0.65490196 0.65490196\n",
      "  0.61960784 0.7372549  0.76862745 0.50196078 0.66666667 0.70196078\n",
      "  0.50196078 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.1372549  0.36078431 0.50588235\n",
      "  0.78039216 0.72156863 0.67058824 0.70196078 0.73333333 0.96470588\n",
      "  0.66666667 0.74117647 0.94117647 0.61176471 0.66666667 0.68235294\n",
      "  0.65098039 0.74509804 0.90196078 0.54509804 0.4        0.54509804\n",
      "  0.34901961 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.55686275 0.83529412 0.65098039 0.59215686 0.69803922 0.92156863\n",
      "  0.74509804 0.63529412 0.95686275 0.70196078 0.71764706 0.62745098\n",
      "  0.55686275 0.64313725 0.81568627 0.49019608 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.00392157 0.00784314\n",
      "  0.         0.37647059 0.70980392 0.70196078 0.69803922 0.69803922\n",
      "  0.7254902  0.62745098 0.81568627 0.67058824 0.65490196 0.65490196\n",
      "  0.72156863 0.68235294 0.41176471 0.         0.00392157 0.00784314\n",
      "  0.00392157 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.00784314 0.         0.00392157 0.20392157 0.3254902  0.38431373\n",
      "  0.43921569 0.5254902  0.57254902 0.41176471 0.34901961 0.28627451\n",
      "  0.19215686 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784) (12000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(-1, 784)\n",
    "x_val = x_val.reshape(-1, 784)\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.to_categorical([0, 1, 3]) # 원 핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 10) (12000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_encoded = tf.keras.utils.to_categorical(y_train)\n",
    "y_val_encoded = tf.keras.utils.to_categorical(y_val)\n",
    "print(y_train_encoded.shape, y_val_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0], y_train_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
